{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preious post, we saw various steps involved in creating a machine learning (ML) model. You might have noticed in Building ML Model we consider multiple Algorithums in a pipeline and then tuned hyperparameter for all the Models. Don't you feel that It would have been easier if some automated tools are there to ease the burden of repetitive and time-consuming tasks of machine learning pipeline design and hyperparameter optimization. \n",
    "\n",
    "Here, comes AutoML taking over the machine learning model-building process: once a data set is in a relatively clean format, the AutoML system will be able to design and optimize a machine learning pipeline faster than 99% of the humans out there.\n",
    "\n",
    "There are many such AutoML tools are available and most popular of them are,\n",
    "\n",
    "- TPOT\n",
    "- H2O\n",
    "- Auto-sklearn \n",
    "- Azure AutoML \n",
    "- etc.. \n",
    "\n",
    "Below we will see an example of TPOT, rest others also work on similar idea. Like Auto-sklearn, TPOT also works on top of scikit-learn, and you can think this as scikit-learn wrapper. It will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.\n",
    "\n",
    "![](tpotFlow.JPG)\n",
    "\n",
    "I am using the Problem statement and Data set what I used in Part-2. But for simplicity we will dillute the some of the Pre-processing steps. Jupyter Notebook file and training/test files can also be downloaded from my [git repository](https://github.com/kumardh/python-practice/tree/master/ML-3). Alright, Let's get started -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e362546d-9580-4b96-a696-65cdf7c29dbc",
    "_uuid": "939dee09a5609a1cacd0b30dc356ddec6070c187"
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b2738823-6a25-443f-91a3-28311d8a24b3",
    "_uuid": "25d3b2548cb11c6f001cd375f3cae145e1ee654f"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "ee34c36a-ee74-4ae6-b845-d5362ba8190a",
    "_uuid": "93749aa6715208ead3060a353c5a7f603cd17e7d"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleanup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "fae561e2-8324-4640-a766-7e32be2450a3",
    "_uuid": "300037c5dd5b20f2c5677e6f999da9a1eefd7c56"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['PassengerId'], axis=1)\n",
    "train_df = train_df.drop(['Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Cabin'], axis=1)\n",
    "train_df = train_df.drop(['Ticket'], axis=1)\n",
    "test_df = test_df.drop(['Ticket'], axis=1)\n",
    "train_df = train_df.drop(['Name'], axis=1)\n",
    "test_df = test_df.drop(['Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "6529c2c6-a04a-4541-9bec-f34b0d5279af",
    "_uuid": "adda6f3f3a574cf10946a1e05f9538edabe4a9e3"
   },
   "outputs": [],
   "source": [
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    mean = train_df[\"Age\"].mean()\n",
    "    std = test_df[\"Age\"].std()\n",
    "    is_null = dataset[\"Age\"].isnull().sum()\n",
    "    # compute random numbers between the mean, std and is_null\n",
    "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "    # fill NaN values in Age column with random values generated\n",
    "    age_slice = dataset[\"Age\"].copy()\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    dataset[\"Age\"] = age_slice\n",
    "    dataset[\"Age\"] = train_df[\"Age\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "ac7c3b55-52ab-4a10-bfd4-28972813a847",
    "_uuid": "6b265174437d7a5363849a3d3772b39618eba7be"
   },
   "outputs": [],
   "source": [
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e4ad0020-5f0a-4223-ad72-0d0189ea0ed1",
    "_uuid": "29044d6922518e777935489fd83861d40c46e18e"
   },
   "source": [
    "## Converting Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "c2805c53-6e47-41e0-b911-4b25e0a704a6",
    "_uuid": "2e75f2b009ed6b803aeaaca6f65a0442eca5add0"
   },
   "outputs": [],
   "source": [
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(0)\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "50b80858-0535-4f44-afc1-a289b971610d",
    "_uuid": "9589bd3170c60b1bf1e22f60853fcf6717ae0f99"
   },
   "outputs": [],
   "source": [
    "genders = {\"male\": 0, \"female\": 1}\n",
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Sex'] = dataset['Sex'].map(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "597ef315-687e-4b90-b41c-306bf04abd65",
    "_uuid": "3a1f417463ad74a7522f66df3d6c52be250d34b7"
   },
   "outputs": [],
   "source": [
    "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map(ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "0278930a-269e-42b0-a935-621004c0da49",
    "_uuid": "904a60da17b1ae9dd27fe4bca993627dd9a8d2cb"
   },
   "outputs": [],
   "source": [
    "data = [train_df, test_df]\n",
    "for dataset in data:\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n",
    "    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n",
    "    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n",
    "    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n",
    "    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "a62a2874-e771-4e16-9504-ab8dab1d518b",
    "_uuid": "7c6e947aac99d71d9db42df3d5d49aaad110a5f2"
   },
   "outputs": [],
   "source": [
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n",
    "    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n",
    "    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a1e3dec4-e257-47bd-b1c2-0a6b72f3321b",
    "_uuid": "66de841b916f0c2a556f43bbfdfbc314e1bea7b7"
   },
   "source": [
    "# **Building Machine Learning Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "fb40cd24-9f0f-4544-b505-902b897fbf0f",
    "_uuid": "6f90f5d59f638f2b1e425b952339a1ae2585a72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=120, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.8327578288714715\n",
      "Generation 2 - Current best internal CV score: 0.8327578288714715\n",
      "Generation 3 - Current best internal CV score: 0.8327578288714715\n",
      "Generation 4 - Current best internal CV score: 0.833931853718029\n",
      "Generation 5 - Current best internal CV score: 0.8395310000365276\n",
      "\n",
      "Best pipeline: RandomForestClassifier(MultinomialNB(input_matrix, alpha=0.1, fit_prior=True), bootstrap=True, criterion=gini, max_features=0.5, min_samples_leaf=4, min_samples_split=17, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "               disable_update_check=False, early_stop=None, generations=5,\n",
       "               max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "               mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "               periodic_checkpoint_folder=None, population_size=20,\n",
       "               random_state=None, scoring=None, subsample=1.0, template=None,\n",
       "               use_dask=False, verbosity=2, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see TPOT has chossen RandomForestClassifier as best fit pipeline.\n",
    "\n",
    "**Classification algorithms and parameters TPOT chooses -**\n",
    "\n",
    "1. ‘sklearn.naive_bayes.BernoulliNB’: { ‘alpha’: [1e-3, 1e-2, 1e-1, 1., 10., 100.], ‘fit_prior’: [True, False] }\n",
    "2. ‘sklearn.naive_bayes.MultinomialNB’: { ‘alpha’: [1e-3, 1e-2, 1e-1, 1., 10., 100.], ‘fit_prior’: [True, False] }\n",
    "3. ‘sklearn.tree.DecisionTreeClassifier’: { ‘criterion’: [“gini”, “entropy”], ‘max_depth’: range(1, 11), ‘min_samples_split’: range(2, 21), ‘min_samples_leaf’: range(1, 21) } \n",
    "4. ‘sklearn.ensemble.ExtraTreesClassifier’: { ‘n_estimators’: [100], ‘criterion’: [“gini”, “entropy”], ‘max_features’: np.arange(0.05, 1.01, 0.05), ‘min_samples_split’: range(2, 21), ‘min_samples_leaf’: range(1, 21), ‘bootstrap’: [True, False] }\n",
    "5. ‘sklearn.ensemble.RandomForestClassifier’: { ‘n_estimators’: [100], ‘criterion’: [“gini”, “entropy”], ‘max_features’: np.arange(0.05, 1.01, 0.05), ‘min_samples_split’: range(2, 21), ‘min_samples_leaf’: range(1, 21), ‘bootstrap’: [True, False] } \n",
    "6. ‘sklearn.ensemble.GradientBoostingClassifier’: { ‘n_estimators’: [100], ‘learning_rate’: [1e-3, 1e-2, 1e-1, 0.5, 1.], ‘max_depth’: range(1, 11), ‘min_samples_split’: range(2, 21), ‘min_samples_leaf’: range(1, 21), ‘subsample’: np.arange(0.05, 1.01, 0.05), ‘max_features’: np.arange(0.05, 1.01, 0.05) }\n",
    "7. ‘sklearn.neighbors.KNeighborsClassifier’: { ‘n_neighbors’: range(1, 101), ‘weights’: [“uniform”, “distance”], ‘p’: [1, 2] } \n",
    "8. ‘sklearn.svm.LinearSVC’: { ‘penalty’: [“l1”, “l2”], ‘loss’: [“hinge”, “squared_hinge”], ‘dual’: [True, False], ‘tol’: [1e-5, 1e-4, 1e-3, 1e-2, 1e-1], ‘C’: [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.] }\n",
    "9. ‘sklearn.linear_model.LogisticRegression’: { ‘penalty’: [“l1”, “l2”], ‘C’: [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.], ‘dual’: [True, False] }\n",
    "10. ‘xgboost.XGBClassifier’: { ‘n_estimators’: [100], ‘max_depth’: range(1, 11), ‘learning_rate’: [1e-3, 1e-2, 1e-1, 0.5, 1.], ‘subsample’: np.arange(0.05, 1.01, 0.05), ‘min_child_weight’: range(1, 21), ‘nthread’: [1] }\n",
    "\n",
    "**Preprocessors that could be applied by TPOT -**\n",
    "\n",
    "1. ‘sklearn.preprocessing.Binarizer’: { ‘threshold’: np.arange(0.0, 1.01, 0.05) }\n",
    "2. ‘sklearn.decomposition.FastICA’: { ‘tol’: np.arange(0.0, 1.01, 0.05) }\n",
    "3. ‘sklearn.cluster.FeatureAgglomeration’: { ‘linkage’: [‘ward’, ‘complete’, ‘average’], ‘affinity’: [‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’, ‘cosine’] }\n",
    "4. ‘sklearn.preprocessing.MaxAbsScaler’: { }\n",
    "5. ‘sklearn.preprocessing.MinMaxScaler’: { }\n",
    "6. ‘sklearn.preprocessing.Normalizer’: { ‘norm’: [‘l1’, ‘l2’, ‘max’] }\n",
    "7. ‘sklearn.kernel_approximation.Nystroem’: { \n",
    "    ‘kernel’: [‘rbf’, ‘cosine’, ‘chi2’, ‘laplacian’, ‘polynomial’, ‘poly’, ‘linear’,  ‘additive_chi2’, ‘sigmoid’], \n",
    "    ‘gamma’: np.arange(0.0, 1.01, 0.05), ‘n_components’: range(1, 11) \n",
    "   }\n",
    "9. ‘sklearn.decomposition.PCA’: { \n",
    "     ‘svd_solver’: [‘randomized’], \n",
    "     ‘iterated_power’: range(1, 11) }\n",
    "10. ‘sklearn.preprocessing.PolynomialFeatures’: { ‘degree’: [2], ‘include_bias’: [False], ‘interaction_only’: [False] }\n",
    "11. ‘sklearn.kernel_approximation.RBFSampler’: { ‘gamma’: np.arange(0.0, 1.01, 0.05) }, \n",
    "12. ‘sklearn.preprocessing.RobustScaler’: { }, \n",
    "13. ‘sklearn.preprocessing.StandardScaler’: { }, \n",
    "14. ‘tpot.builtins.ZeroCount’: { }, \n",
    "15. ‘tpot.builtins.OneHotEncoder’: { ‘minimum_fraction’: [0.05, 0.1, 0.15, 0.2, 0.25], ‘sparse’: [False] } (emphasis mine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e19f656a-703f-42fc-8b99-0f551838483c",
    "_uuid": "36870b352b483fffdba9abbcbcd3aba7dd1f5033"
   },
   "source": [
    "# **Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "5b0bc222-bc3f-4c4b-9d5f-d72844381512",
    "_uuid": "d86495c44cbc9351627d8008c0552056fa1d33ab"
   },
   "outputs": [],
   "source": [
    "Y_prediction = tpot.predict(X_test)\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_prediction\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Limitation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running TPOT isn’t as simple as fitting one model on the dataset. It is considering multiple machine learning algorithms (random forests, linear models, SVMs, etc.) in a pipeline with numerous preprocessing steps (missing value imputation, scaling, PCA, feature selection, etc.), the hyper-parameters for all of the models and preprocessing steps, as well as multiple ways to ensemble or stack the algorithms within the pipeline. That’s why it usually takes a long time to execute and isn’t feasible for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "110281a9-853e-4bd1-9c6b-71a5b95aa90a",
    "_uuid": "2509aa1431e7542d2e4ffcc6b6df853f224dab68"
   },
   "source": [
    "# **Summary**\n",
    "\n",
    "All the methods of AutoML are developed to support data scientists, not to replace them. Such methods can free the data scientist from complicated tasks that can be solved better by machines. But analysing and drawing conclusions still has to be done by data scientists who also knows the application domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
